import time
import random
from pathlib import Path

import streamlit as st
import torch
import numpy as np
from api import gen
#from model import run

BASE_DIR = Path(__file__).resolve().parent

st.set_page_config(page_title="ëŸ­í‚¤ë¹„í‚¤ ì±—ë´‡", page_icon="ğŸ‘¦ğŸ»")
st.header("ëŸ­í‚¤ë¹„í‚¤ ì±—ë´‡", anchor="top", divider="rainbow")

# st.image(str(BASE_DIR.joinpath("assets", "boyfriend.jpeg")), width=200)


def seed_everything(seed):
    torch.manual_seed(seed)  # torchë¥¼ ê±°ì¹˜ëŠ” ëª¨ë“  ë‚œìˆ˜ë“¤ì˜ ìƒì„±ìˆœì„œë¥¼ ê³ ì •í•œë‹¤
    torch.cuda.manual_seed(seed)  # cudaë¥¼ ì‚¬ìš©í•˜ëŠ” ë©”ì†Œë“œë“¤ì˜ ë‚œìˆ˜ì‹œë“œëŠ” ë”°ë¡œ ê³ ì •í•´ì¤˜ì•¼í•œë‹¤
    torch.cuda.manual_seed_all(seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True  # ë”¥ëŸ¬ë‹ì— íŠ¹í™”ëœ CuDNNì˜ ë‚œìˆ˜ì‹œë“œë„ ê³ ì •
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)  # numpyë¥¼ ì‚¬ìš©í•  ê²½ìš° ê³ ì •
    random.seed(seed)  # íŒŒì´ì¬ ìì²´ ëª¨ë“ˆ random ëª¨ë“ˆì˜ ì‹œë“œ ê³ ì •


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        if message["role"] != "system":
            st.markdown(message["content"])

if prompt := st.chat_input("í•˜ê³ ì‹¶ì€ ë§ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    print(prompt)
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
    
        seed_everything(42)
        stream = gen(prompt)
        with st.spinner("ìƒì„±ì¤‘...."):
            time.sleep(5)

        chunks = []
        for chunk in stream:
            chunks.append(chunk)
            message_placeholder.markdown("".join(chunks))
            time.sleep(0.02)
        #message_placeholder.markdown(stream)
    st.session_state.messages.append({"role": "assistant", "content": stream})